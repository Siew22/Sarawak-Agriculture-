import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models
from torch.utils.data import DataLoader, Dataset
import os
import json
from tqdm import tqdm
import time
import numpy as np
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2

# --- 12GB VRAM è‡ªç ”æ——èˆ°é…ç½® ---
DATA_DIR = '../../PlantVillage-Dataset/raw/color'
MODEL_SAVE_PATH = '../../models_store/true_scratch_model_b2_v1.pth' # å…¨æ–°å‘½åï¼Œä»£è¡¨çº¯è¡€è‡ªç ”v1ç‰ˆ
LABELS_PATH = '../../models_store/disease_labels.json'

# --- æ›´æ¿€è¿›çš„å‚æ•°ï¼Œæ—¨åœ¨æ‰“ç ´â€œæ¨¡å¼å´©æºƒâ€ ---
BATCH_SIZE = 32
NUM_WORKERS = 4
NUM_EPOCHS = 60  # ä»é›¶è®­ç»ƒéœ€è¦æ›´å……åˆ†çš„æ—¶é—´æ¥å­¦ä¹ 
LEARNING_RATE = 0.001
MODEL_ARCHITECTURE = 'efficientnet_b2'

class PlantVillageDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œä»¥æ— ç¼é›†æˆ Albumentations å›¾åƒå¢å¼ºåº“ã€‚"""
    def __init__(self, root_dir, transform=None):
        self.dataset = datasets.ImageFolder(root=root_dir)
        self.transform = transform
        self.classes = self.dataset.classes
        self.class_to_idx = self.dataset.class_to_idx

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        image = np.array(image) # Albumentations éœ€è¦ NumPy æ•°ç»„æ ¼å¼
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        return image, label

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == 'cpu': 
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDA, è®­ç»ƒä¼šéå¸¸æ…¢ã€‚")
    else: 
        print(f"âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")

    try:
        with open(LABELS_PATH, 'r') as f:
            NUM_CLASSES = len(json.load(f))
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ ‡ç­¾æ–‡ä»¶ '{LABELS_PATH}' æœªæ‰¾åˆ°ï¼è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
        return
    print(f"âœ… æ•°æ®é›†å…±æœ‰ {NUM_CLASSES} ä¸ªç±»åˆ«ã€‚")

    # --- ç¬¬ä¸€å‘³ç«ï¼šæé™æ•°æ®å¢å¼º (Albumentations) ---
    data_transforms = {
        'train': A.Compose([
            # --- â†“â†“â†“ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ–°çš„å‡½æ•°è°ƒç”¨è¯­æ³• â†“â†“â†“ ---
            A.RandomResizedCrop(size=(260, 260), scale=(0.7, 1.0), p=1.0),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomRotate90(p=0.5),
            A.OneOf([
                A.ISONoise(p=1.0),
                A.GaussNoise(p=1.0),
            ], p=0.2),
            A.OneOf([
                A.RandomBrightnessContrast(p=1.0),
                A.HueSaturationValue(p=1.0),
            ], p=0.3),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ]),
        # ... (val éƒ¨åˆ†çš„ä»£ç æ— éœ€ä¿®æ”¹)
        'val': A.Compose([
            A.Resize(288, 288),
            A.CenterCrop(260, 260),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ]),
    }

    # --- ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ç±» ---
    full_dataset = PlantVillageDataset(root_dir=DATA_DIR)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    # ä½¿ç”¨ç”Ÿæˆå™¨ç¡®ä¿æ¯æ¬¡è¿è¡Œçš„åˆ’åˆ†éƒ½ä¸åŒï¼Œå¢åŠ éšæœºæ€§
    generator = torch.Generator().manual_seed(42) 
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size], generator=generator)

    # ä¸ºSubsetåº”ç”¨transformçš„æ­£ç¡®æ–¹å¼
    train_dataset.dataset.transform = data_transforms['train']
    val_dataset.dataset.transform = data_transforms['val']
    
    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True),
        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
    }

    print(f"æ­£åœ¨æ„å»ºä¸€ä¸ªå…¨æ–°çš„ '{MODEL_ARCHITECTURE}' æ¨¡å‹ (100% å®Œå…¨è‡ªç ”)...")
    # --- æ ¸å¿ƒä¿è¯ï¼šweights=None ---
    model = models.efficientnet_b2(weights=None, num_classes=NUM_CLASSES)
    model = model.to(device)

    # --- ç¬¬äºŒå‘³ç«ï¼šæ›´å¼ºçš„æ­£åˆ™åŒ– ---
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)
    
    # --- ç¬¬ä¸‰å‘³ç«ï¼šæ›´æ™ºèƒ½çš„å­¦ä¹ ç‡ç­–ç•¥ ---
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - 5, eta_min=1e-6)
    
    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))
    start_time = time.time()
    best_acc = 0.0

    # --- è®­ç»ƒä¸»å¾ªç¯ ---
    print("\n--- å¼€å§‹â€œä¸‰å‘³çœŸç«â€ç‚¼ä¸¹ ---")
    for epoch in range(NUM_EPOCHS):
        print(f'\nEpoch {epoch+1}/{NUM_EPOCHS} | å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.6f}')
        print('-' * 25)

        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            running_loss = 0.0
            running_corrects = 0
            
            progress_bar = tqdm(dataloaders[phase], desc=f"{phase.capitalize()} Epoch {epoch+1}")
            for inputs, labels in progress_bar:
                inputs = inputs.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)

                with torch.set_grad_enabled(phase == 'train'):
                    with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                    if phase == 'train':
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                progress_bar.set_postfix(loss=f'{loss.item():.4f}')

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), MODEL_SAVE_PATH)
                print(f"ğŸ‰ æ–°çš„æœ€ä½³è‡ªç ”æ——èˆ°æ¨¡å‹å·²ä¿å­˜ (Accuracy: {best_acc:.4f}) ğŸ‰")
        
        # åœ¨æ¯ä¸ªepochç»“æŸåæ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

    time_elapsed = time.time() - start_time
    print(f'\n--- ç‚¼ä¸¹å®Œæˆ ---')
    print(f'æ€»è€—æ—¶: {time_elapsed // 60:.0f}åˆ† {time_elapsed % 60:.0f}ç§’')
    print(f'ğŸ† æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:4f}')

if __name__ == "__main__":
    train()