# train/train_model.py (6GB VRAM - ä¼˜åŒ–å¢å¼ºç‰ˆ)
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models
from torch.utils.data import DataLoader, Dataset
import os
import json
from tqdm import tqdm
import time
import numpy as np
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2

# --- 6GB VRAM è‡ªç ”ä¼˜åŒ–é…ç½® ---
DATA_DIR = '../../PlantVillage-Dataset/raw/color'
MODEL_SAVE_PATH = '../../models_store/scratch_model_b0_6gb_v2.pth' # æ–°ç‰ˆæœ¬å‘½å
LABELS_PATH = '../../models_store/disease_labels.json'

# --- ä¸º6GB VRAM ç²¾å¿ƒè°ƒæ ¡çš„å‚æ•° ---
BATCH_SIZE = 16  # ä¿æŒä¸€ä¸ªå®‰å…¨çš„æ‰¹é‡å¤§å°
NUM_WORKERS = 2  # å‡å°‘CPUå’Œå†…å­˜å‹åŠ›
NUM_EPOCHS = 50  # ä»é›¶è®­ç»ƒéœ€è¦è¶³å¤Ÿçš„è½®æ¬¡
LEARNING_RATE = 0.001
MODEL_ARCHITECTURE = 'efficientnet_b0'

# è‡ªå®šä¹‰Datasetç±»ä»¥é›†æˆAlbumentations
class PlantVillageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.dataset = datasets.ImageFolder(root=root_dir)
        self.transform = transform
        self.classes = self.dataset.classes

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        image = np.array(image)
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        return image, label

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == 'cpu': 
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDA, è®­ç»ƒä¼šéå¸¸æ…¢ã€‚")
    else: 
        print(f"âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")

    try:
        with open(LABELS_PATH, 'r') as f:
            NUM_CLASSES = len(json.load(f))
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ ‡ç­¾æ–‡ä»¶ '{LABELS_PATH}' æœªæ‰¾åˆ°ï¼")
        return
    print(f"âœ… æ•°æ®é›†å…±æœ‰ {NUM_CLASSES} ä¸ªç±»åˆ«ã€‚")

    # --- ä¸“ä¸º6GB VRAMä¼˜åŒ–çš„æ•°æ®å¢å¼ºç­–ç•¥ ---
    data_transforms = {
        'train': A.Compose([
            # --- â†“â†“â†“ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ–°çš„å‡½æ•°è°ƒç”¨è¯­æ³• â†“â†“â†“ ---
            A.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2), 
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ]),
        # ... (val éƒ¨åˆ†çš„ä»£ç æ— éœ€ä¿®æ”¹)
        'val': A.Compose([
            A.Resize(256, 256),
            A.CenterCrop(224, 224),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ]),
    }

    full_dataset = PlantVillageDataset(root_dir=DATA_DIR)
    
    # åˆ’åˆ†æ•°æ®é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    generator = torch.Generator().manual_seed(42)
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size], generator=generator)

    # åº”ç”¨transform
    train_dataset.dataset.transform = data_transforms['train']
    val_dataset.dataset.transform = data_transforms['val']
    
    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True),
        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
    }

    print(f"æ­£åœ¨æ„å»ºä¸€ä¸ªå…¨æ–°çš„ '{MODEL_ARCHITECTURE}' æ¨¡å‹ (100% å®Œå…¨è‡ªç ”)...")
    # --- æ ¸å¿ƒä¿è¯ï¼šweights=None ---
    model = models.efficientnet_b0(weights=None, num_classes=NUM_CLASSES)
    model = model.to(device)

    # --- å¼•å…¥ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥ ---
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)
    
    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))
    start_time = time.time()
    best_acc = 0.0

    # --- è®­ç»ƒä¸»å¾ªç¯ ---
    print("\n--- å¼€å§‹ä¼˜åŒ–ç‰ˆè®­ç»ƒ ---")
    for epoch in range(NUM_EPOCHS):
        print(f'\nEpoch {epoch+1}/{NUM_EPOCHS} | å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.6f}')
        print('-' * 25)

        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            running_loss = 0.0
            running_corrects = 0
            
            progress_bar = tqdm(dataloaders[phase], desc=f"{phase.capitalize()} Epoch {epoch+1}")
            for inputs, labels in progress_bar:
                inputs = inputs.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)

                with torch.set_grad_enabled(phase == 'train'):
                    with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                    if phase == 'train':
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                progress_bar.set_postfix(loss=f'{loss.item():.4f}')

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), MODEL_SAVE_PATH)
                print(f"ğŸ‰ æ–°çš„æœ€ä½³è‡ªç ”ç»æµç‰ˆæ¨¡å‹å·²ä¿å­˜ (Accuracy: {best_acc:.4f}) ğŸ‰")
        
        scheduler.step()

    time_elapsed = time.time() - start_time
    print(f'\n--- è®­ç»ƒå®Œæˆ ---')
    print(f'æ€»è€—æ—¶: {time_elapsed // 60:.0f}åˆ† {time_elapsed % 60:.0f}ç§’')
    print(f'ğŸ† æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:4f}')

if __name__ == "__main__":
    train()